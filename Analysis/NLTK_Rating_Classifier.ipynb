{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diplomatic Event Rating Classifier\n",
    "Adapted from a Scikit-Learn SVM classifier by Prof. Bengfort, trained on the newspaper articles published 7 days prior to the events.<br>\n",
    "Pipeline: NLTK Preprocessor, TF-IDF Vectorizer, SGD Classifier<br>\n",
    "See https://github.com/bbengfort/bbengfort.github.io/blob/master/_posts/2016-05-19-text-classification-nltk-sckit-learn.md\n",
    "<br>Local file needed: RatingsandText_EventDate.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms input data by using NLTK tokenization, lemmatization, and\n",
    "    other normalization and filtering techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        \"\"\"\n",
    "        Instantiates the preprocessor, which make load corpora, models, or do\n",
    "        other time-intenstive NLTK data loading.\n",
    "        \"\"\"\n",
    "        stoplist = sw.words('english')\n",
    "        additionalsw = [u'one', u'first', u'two', u'second', u'three', u'third', u'four',\n",
    "                        u'fourth', u'five', u'fifth', u'six', u'sixth', u'seven', u'seventh', \n",
    "                        u'eight', u'eighth', u'nine', u'ninth', u'ten', u'tenth', u'rsquo', \n",
    "                        u'rdquo', u'lsquo', u'ldquo', u'syria', u'obama']\n",
    "        stoplist += additionalsw\n",
    "\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = set(stopwords) if stopwords else set(stoplist)\n",
    "        self.punct      = set(punct) if punct else set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit simply returns self, no other information is needed.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        No inverse transformation\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Actually runs the preprocessing on each document.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If punctuation or stopword, ignore token and continue\n",
    "                if token in self.stopwords or all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"\n",
    "        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "        tag to perform much more accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_classification_report(cr, title='Classification report', cmap=plt.cm.Reds):\n",
    "\n",
    "    lines = cr.split('\\n')\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "\n",
    "    for line in lines[2 : (len(lines) - 3)]:\n",
    "        t = line.split()\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        plotMat.append(v)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig = plt.imshow(plotMat, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    for c in range(len(plotMat)+1):\n",
    "        for r in range(len(classes)):\n",
    "            try:\n",
    "                txt = plotMat[r][c]\n",
    "                ax.text(c,r,plotMat[r][c],va='center',ha='center')\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    x_tick_marks = np.arange(3)\n",
    "    y_tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(x_tick_marks, ['precision', 'recall', 'f1-score'], rotation=45)\n",
    "    plt.yticks(y_tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Classes')\n",
    "    plt.xlabel('Measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Builds a classifer for the given list of documents and targets in two\n",
    "    stages: the first does a train/test split and prints a classifier report,\n",
    "    the second rebuilds the model on the entire corpus and returns it for\n",
    "    operationalization.\n",
    "\n",
    "    X: a list or iterable of raw strings, each representing a document.\n",
    "    y: a list or iterable of labels, which will be label encoded.\n",
    "\n",
    "    Can specify the classifier to build with: if a class is specified then\n",
    "    this will build the model with the Scikit-Learn defaults, if an instance\n",
    "    is given, then it will be used directly in the build pipeline.\n",
    "\n",
    "    If outpath is given, this function will write the model as a pickle.\n",
    "    If verbose, this function will print out information to the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier()\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    if verbose: print(\"Building for evaluation\")\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)\n",
    "    model, secs = build(classifier, X_train, y_train)\n",
    "\n",
    "    if verbose: print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "    if verbose: print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "    \n",
    "    plot_classification_report(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "\n",
    "    if verbose: print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "\n",
    "    if verbose: print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        f.close()\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    \"\"\"\n",
    "    Accepts a Pipeline with a classifer and a TfidfVectorizer and computes\n",
    "    the n most informative features of the model. If text is given, then will\n",
    "    compute the most informative features for classifying that text.\n",
    "\n",
    "    Note that this function will only work on linear models with coefs_\n",
    "    \"\"\"\n",
    "\n",
    "    catdict = {0: '3: aggressive', 1: '0: friendly', 2: '1: neutral', 3: '2: threatening'}\n",
    "\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {} model.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=True\n",
    "    )\n",
    "\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        category = model.predict([text])\n",
    "        output.append(\"Classified as: {}\".format(catdict[category[0]]))\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(cp, fnp, cn, fnn)\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the article text and ratings\n",
    "\n",
    "datatable = pd.read_csv('RatingsandText_EventDate.csv', encoding='utf-8')\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for index, row in datatable.iterrows():\n",
    "    data.append(row['Title'] + ' ' + row['Text'])\n",
    "    if row['FinalRating'] == 0:\n",
    "        target.append('friendly')\n",
    "    if row['FinalRating'] == 1:\n",
    "        target.append('neutral')\n",
    "    if row['FinalRating'] == 2:\n",
    "        target.append('threatening')\n",
    "    if row['FinalRating'] == 3:\n",
    "        target.append('aggressive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n",
      "Evaluation model fit in 276.021 seconds\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " aggressive       0.57      0.71      0.63       445\n",
      "   friendly       1.00      0.39      0.56        23\n",
      "    neutral       0.63      0.53      0.57       271\n",
      "threatening       0.60      0.50      0.55       346\n",
      "\n",
      "avg / total       0.60      0.59      0.59      1085\n",
      "\n",
      "Building complete model and saving ...\n"
     ]
    }
   ],
   "source": [
    "PATH = \"model.pickle\"\n",
    "\n",
    "if not os.path.exists(PATH):\n",
    "    # Build the model\n",
    "    X = data\n",
    "    y = target\n",
    "\n",
    "    model = build_and_evaluate(X,y, outpath=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7299          paris    -2.7465 representative\n",
      "3.0368            oct    -2.7160         carter\n",
      "2.5465           2014    -2.5103          hagel\n",
      "2.5177        donilon    -2.3881          plane\n",
      "2.3223           fear    -2.3403         barack\n",
      "2.2537      diplomacy    -2.1972     deployment\n",
      "2.2261        panetta    -2.1756            win\n",
      "2.1907            kim    -2.1721            akp\n",
      "2.1107           wage    -2.1662        downing\n",
      "2.0945          rally    -2.1324        tension\n",
      "2.0819        embargo    -2.1213           move\n",
      "2.0533  establishment    -2.1145            feb\n",
      "2.0362     government    -2.1054     parliament\n",
      "2.0146     resistance    -2.1007          cairo\n",
      "2.0083          annan    -2.0957         leader\n",
      "2.0000    significant    -2.0934          world\n",
      "1.9960          fahmy    -2.0828   demonstrator\n",
      "1.9566          board    -2.0737        project\n",
      "1.9472           plot    -2.0722           aumf\n",
      "1.9124      stockpile    -2.0496           head\n"
     ]
    }
   ],
   "source": [
    "with open(PATH, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(show_most_informative_features(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d50f8a046f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_classification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "plot_classification_report(clsr(y_test, y_pred, target_names=labels.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
